{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18c24aa-fc32-44a7-9851-74a0b8d81999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers\n",
    "# ! pip install datasets\n",
    "# ! pip install pandas\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# ! pip install matplotlib\n",
    "# !pip3 install adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ab0ba6-c427-459f-a75f-2022a67ea316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification, get_scheduler\n",
    "# from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from transformers import get_scheduler\n",
    "from adamp import AdamP\n",
    "from functools import partial\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd3551-dd6e-47f0-8620-aa6e41531685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "16 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "batch_size_val = 4 if device == \"cpu\" else 16 if device == \"cuda\" else 16\n",
    "num_workers_val = 4 if device == \"cuda\" else 0\n",
    "print(batch_size_val, num_workers_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a896a2f7-623e-4580-b0c0-e7a6d8eef2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "base_model = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset('ag_news', split='train', cache_dir='./data/')\n",
    "test_dataset = load_dataset('ag_news', split='test', cache_dir='./data/')\n",
    "\n",
    "# Extract the number of classess and their names\n",
    "num_labels = train_dataset.features['label'].num_classes\n",
    "class_names = train_dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Rename label column\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(tokenized_train, batch_size=batch_size_val, shuffle=True, collate_fn=data_collator, num_workers=num_workers_val)\n",
    "test_dataloader = DataLoader(tokenized_test, batch_size=batch_size_val, shuffle=False, collate_fn=data_collator, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5c7225-d698-4b05-aeb5-48ebff5535bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lora_model(base_model, r, alpha):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        id2label=id2label,\n",
    "        cache_dir=\"./model_dir\"\n",
    "    )\n",
    "\n",
    "    # Freeze base model params\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define LoRA layers\n",
    "    class LoRALayer(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim, r, alpha):\n",
    "            super().__init__()\n",
    "            self.A = nn.Parameter(torch.empty(r, in_dim))\n",
    "            self.B = nn.Parameter(torch.empty(out_dim, r))\n",
    "            torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "            torch.nn.init.zeros_(self.B)\n",
    "            self.scaling = alpha / r\n",
    "\n",
    "        def forward(self, x):\n",
    "            A = self.A.to(x.device)\n",
    "            B = self.B.to(x.device)\n",
    "            return self.scaling * (x @ A.T @ B.T)\n",
    "\n",
    "    class LinearWithLoRA(nn.Module):\n",
    "        def __init__(self, linear, r, alpha):\n",
    "            super().__init__()\n",
    "            self.linear = linear\n",
    "            self.lora = LoRALayer(linear.in_features, linear.out_features, r, alpha)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear(x) + self.lora(x)\n",
    "\n",
    "    assign_lora = partial(LinearWithLoRA, r=r, alpha=alpha)\n",
    "\n",
    "    # Adding interjecting parameters to other layers:\n",
    "    for layer in model.roberta.encoder.layer:\n",
    "        layer.attention.self.query = assign_lora(layer.attention.self.query)\n",
    "        layer.attention.self.value = assign_lora(layer.attention.self.value)\n",
    "        layer.output.dense = assign_lora(layer.output.dense)\n",
    "    \n",
    "    if r <= 4:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"classifier\" in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197a4592-5d20-4434-8fa0-5b0f6c06eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3be9ff-8ab5-4cde-b973-ec0777f7664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, targets):\n",
    "  predictions = torch.log_softmax(y_pred, dim=1).argmax(dim=1)\n",
    "  accuracy = (predictions == targets).sum() / len(targets)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496f4af5-6952-4ebf-8d8f-7067dfb847eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = setup_lora_model(base_model, r=4, alpha=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e72d9c6a-2fb9-42d4-9973-b90edec521e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "925444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a41b34-5328-4cc5-a900-4a7cc5973dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3851827e-c00b-46b2-b78d-0b6b627b903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, checkpoint_path, resume_training=True):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if resume_training:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        step = checkpoint.get('step', 0)\n",
    "        epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"Resumed training from epoch {epoch}, step {step}\")\n",
    "        return model, optimizer, step, epoch\n",
    "    else:\n",
    "        print(\"Model loaded for inference\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800cea6e-3650-4ca9-b369-524d1a860275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(train_data, test_data, title, ylabel, save_path_base):\n",
    "    steps, train_vals = zip(*train_data)\n",
    "    _, test_vals = zip(*test_data)\n",
    "\n",
    "    # Plot with interactivity\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(steps, train_vals, label='Train')\n",
    "    ax.plot(steps, test_vals, label='Test')\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Save interactive plot as HTML\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        import plotly.io as pio\n",
    "\n",
    "        fig_plotly = go.Figure()\n",
    "        fig_plotly.add_trace(go.Scatter(x=steps, y=train_vals, mode='lines+markers', name='Train'))\n",
    "        fig_plotly.add_trace(go.Scatter(x=steps, y=test_vals, mode='lines+markers', name='Test'))\n",
    "        fig_plotly.update_layout(title=title, xaxis_title='Steps', yaxis_title=ylabel)\n",
    "        interactive_path = save_path_base.replace(\".png\", \".html\")\n",
    "        pio.write_html(fig_plotly, file=interactive_path, auto_open=False)\n",
    "    except ImportError:\n",
    "        print(\"Plotly not installed — skipping interactive plot save.\")\n",
    "\n",
    "    # Save static plot\n",
    "    static_path = save_path_base.replace(\".html\", \".png\")\n",
    "    fig.savefig(static_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save raw data\n",
    "    df = pd.DataFrame({\n",
    "        \"Step\": steps,\n",
    "        \"Train\": train_vals,\n",
    "        \"Test\": test_vals\n",
    "    })\n",
    "    csv_path = save_path_base.replace(\".png\", \"_data.csv\").replace(\".html\", \"_data.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved plot to {static_path} and interactive/data files to {interactive_path if 'interactive_path' in locals() else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "147bcaf5-7a2e-44e4-827c-4c8489fd417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, lr_scheduler, save_every_steps=200, output_dir=\"./checkpoints\", epochs=None, max_steps=None):\n",
    "    import time, os, torch\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    recent_checkpoints = []\n",
    "\n",
    "    total_training_time = 0\n",
    "    total_steps = 0\n",
    "    epoch = 0\n",
    "    train_losses, train_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    log_every_steps = 500  # how often to log + evaluate\n",
    "\n",
    "    print(\"Starting training...\\n\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    while True:\n",
    "        if epochs is not None and epoch >= epochs:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        epoch_steps = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            if max_steps is not None and total_steps >= max_steps:\n",
    "                break\n",
    "\n",
    "            step_start = time.time()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            acc = get_accuracy(logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_steps += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            if total_steps % log_every_steps == 0 or total_steps == max_steps:\n",
    "                avg_train_loss = epoch_loss / epoch_steps\n",
    "                avg_train_acc = epoch_acc / epoch_steps\n",
    "                train_losses.append((total_steps, avg_train_loss))\n",
    "                train_accuracies.append((total_steps, avg_train_acc))\n",
    "            \n",
    "                # Evaluate on test set\n",
    "                test_loss, test_acc = evaluate(model, test_dataloader, return_values=True)\n",
    "                test_losses.append((total_steps, test_loss))\n",
    "                test_accuracies.append((total_steps, test_acc))\n",
    "            \n",
    "            loss.backward()\n",
    "            # Grad Norm Clipping: \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "            \n",
    "            step_end = time.time()\n",
    "            step_time = step_end - step_start\n",
    "\n",
    "            if total_steps % 250 == 0 or total_steps == 1:\n",
    "                avg_loss = epoch_loss / epoch_steps\n",
    "                avg_acc = epoch_acc / epoch_steps\n",
    "                print(f\"[Step {total_steps}] Avg Loss (epoch): {avg_loss:.4f} | Avg Acc (epoch): {avg_acc:.4f} | Step Time: {step_time:.2f}s\")\n",
    "\n",
    "            if total_steps % save_every_steps == 0 or total_steps == max_steps:\n",
    "                ckpt_path = os.path.join(output_dir, f\"model_step_{total_steps}.pt\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'step': total_steps,\n",
    "                    'epoch': epoch,\n",
    "                }, ckpt_path)\n",
    "\n",
    "                print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "                recent_checkpoints.append(ckpt_path)\n",
    "                if len(recent_checkpoints) > 1:\n",
    "                    old_ckpt = recent_checkpoints.pop(0)\n",
    "                    if os.path.exists(old_ckpt):\n",
    "                        os.remove(old_ckpt)\n",
    "                        print(f\"Old checkpoint removed: {old_ckpt}\")\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        # Per-epoch average loss & accuracy\n",
    "        avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0\n",
    "        avg_epoch_acc = epoch_acc / epoch_steps if epoch_steps > 0 else 0\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  Avg Accuracy: {avg_epoch_acc:.4f}\")\n",
    "        print(f\"  Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        if max_steps is not None and total_steps >= max_steps:\n",
    "            print(f\"Reached max_steps={max_steps}, stopping training.\")\n",
    "            break\n",
    "    ckpt_path = os.path.join(output_dir, f\"model_step_{total_steps}_final.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step': total_steps,\n",
    "        'epoch': epoch,\n",
    "    }, ckpt_path)\n",
    "    print(f\"Final checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "    overall_time = time.time() - training_start\n",
    "    print(f\"Training completed in {epoch} epoch(s)\")\n",
    "    print(f\"Total training time: {overall_time:.2f}s\")\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb5db7c-e1fa-4987-815a-ab067a708ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, return_values=False):\n",
    "    interval = len(test_loader) // 5 if len(test_loader) >= 5 else 1\n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            acc = get_accuracy(logits, labels)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_acc += acc.item()\n",
    "\n",
    "            if (batch_idx + 1) % interval == 0:\n",
    "                print(f\"Batch: {batch_idx+1}/{len(test_loader)} | Test loss: {loss:.4f} | accuracy: {acc:.4f}\")\n",
    "\n",
    "    test_loss = total_test_loss / len(test_loader)\n",
    "    test_acc = total_test_acc / len(test_loader)\n",
    "\n",
    "    print(f\"Test loss: {test_loss:.4f} acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "    if return_values:\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "559a6b21-87e8-4267-aab8-a5f2a63266f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unlabelled(model, data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits  # [B, num_classes]\n",
    "            predictions = torch.argmax(logits, dim=1)  # get predicted class indices\n",
    "            preds.append(predictions.cpu())\n",
    "\n",
    "    return torch.cat(preds, dim=0)  # combine into a single tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec4d47a3-6d08-4706-81e6-5f13c4319059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 8000/8000 [00:01<00:00, 5075.07 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LoRA r4_alpha12 ===\n",
      "Trainable Parameter Count: 925444\n",
      "Starting training...\n",
      "\n",
      "[Step 1] Avg Loss (epoch): 1.3838 | Avg Acc (epoch): 0.1875 | Step Time: 0.35s\n",
      "[Step 250] Avg Loss (epoch): 1.3377 | Avg Acc (epoch): 0.4273 | Step Time: 0.27s\n",
      "Batch: 95/475 | Test loss: 0.4797 | accuracy: 0.9375\n",
      "Batch: 190/475 | Test loss: 0.3476 | accuracy: 0.9375\n",
      "Batch: 285/475 | Test loss: 0.5747 | accuracy: 0.7500\n",
      "Batch: 380/475 | Test loss: 0.5372 | accuracy: 0.9375\n",
      "Batch: 475/475 | Test loss: 0.5607 | accuracy: 0.9375\n",
      "Test loss: 0.5362 acc: 0.8804\n",
      "\n",
      "[Step 500] Avg Loss (epoch): 1.0172 | Avg Acc (epoch): 0.6538 | Step Time: 45.08s\n",
      "Checkpoint saved: ./results/r4_alpha12/checkpoints/model_step_500.pt\n",
      "\n",
      "Epoch 1 Summary:\n",
      "  Avg Loss: 1.0172\n",
      "  Avg Accuracy: 0.6538\n",
      "  Epoch Time: 183.72s\n",
      "\n",
      "Reached max_steps=500, stopping training.\n",
      "Final checkpoint saved: ./results/r4_alpha12/checkpoints/model_step_500_final.pt\n",
      "Training completed in 1 epoch(s)\n",
      "Total training time: 184.48s\n",
      "=== Inference for r4_alpha12 ===\n",
      "Predictions saved to ./results/inference_output_r4_alpha12.csv\n",
      "Inference time: 47.21s\n",
      "Plotly not installed — skipping interactive plot save.\n",
      "Saved plot to ./results/r4_alpha12/loss_plot_r4_alpha12.png and interactive/data files to N/A\n",
      "Plotly not installed — skipping interactive plot save.\n",
      "Saved plot to ./results/r4_alpha12/accuracy_plot_r4_alpha12.png and interactive/data files to N/A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "lora_r_values = [4]\n",
    "lora_alpha_values = [12]\n",
    "max_steps = 500\n",
    "save_every_steps = 1500\n",
    "max_epochs = None\n",
    "if max_epochs is not None:\n",
    "    num_training_steps = len(train_dataloader) * max_epochs\n",
    "else:\n",
    "    num_training_steps = max_steps\n",
    "\n",
    "\n",
    "# Load unlabelled test set\n",
    "unlabelled_df = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "tokenized_unlabelled = unlabelled_df.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_unlabelled.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "unlabelled_loader = DataLoader(tokenized_unlabelled, batch_size=batch_size_val, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "for r, alpha in zip(lora_r_values, lora_alpha_values):\n",
    "    tag = f\"r{r}_alpha{alpha}\"\n",
    "    print(f\"\\n=== Training LoRA {tag} ===\")\n",
    "    model = setup_lora_model(base_model, r, alpha)\n",
    "    print(\"Trainable Parameter Count: {}\".format(count_parameters(model)))\n",
    "    \n",
    "    optimizer = AdamP(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=int(num_training_steps * 0.1),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    ckpt_dir = f\"./results/{tag}/checkpoints/\"\n",
    "    # train_losses, train_accuracies, test_losses, test_accuracies = train(model, train_dataloader, optimizer, save_every_steps=save_every_steps, output_dir=ckpt_dir, max_steps=max_steps)\n",
    "    train_losses, train_accuracies, test_losses, test_accuracies = train(model, train_dataloader, optimizer, save_every_steps=save_every_steps, output_dir=ckpt_dir, epochs=max_epochs, max_steps=max_steps, lr_scheduler=lr_scheduler)\n",
    "    # evaluate(model, test_dataloader, return_values=False)\n",
    "    print(f\"=== Inference for {tag} ===\")\n",
    "    inference_start = time.time()\n",
    "    preds = evaluate_unlabelled(model, unlabelled_loader)\n",
    "\n",
    "    output_dir = \"./results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"inference_output_{tag}.csv\")\n",
    "\n",
    "    df_output = pd.DataFrame({\n",
    "      'ID': range(len(preds)),\n",
    "      'Label': preds.numpy()\n",
    "    })\n",
    "    df_output.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "    inference_end = time.time()\n",
    "    inference_time = inference_end - inference_start\n",
    "    print(f\"Inference time: {inference_time:.2f}s\")\n",
    "    \n",
    "    plot_dir = f\"./results/{tag}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    plot_metrics(train_losses, test_losses, f\"{tag} - Loss\", \"Loss\", f\"./results/{tag}/loss_plot_{tag}.png\")\n",
    "    plot_metrics(train_accuracies, test_accuracies, f\"{tag} - Accuracy\", \"Accuracy\", f\"./results/{tag}/accuracy_plot_{tag}.png\")\n",
    "\n",
    "    # Cleanup after inference\n",
    "    del train_losses, train_accuracies, test_losses, test_accuracies\n",
    "    del model\n",
    "    del optimizer\n",
    "    del preds\n",
    "    del df_output\n",
    "    torch.cuda.empty_cache()  # safe to call on CPU too\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dfa9b92-9dff-4986-bd2e-d1f3d37c49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for inference\n",
      "Batch: 95/475 | Test loss: 0.4797 | accuracy: 0.9375\n",
      "Batch: 190/475 | Test loss: 0.3476 | accuracy: 0.9375\n",
      "Batch: 285/475 | Test loss: 0.5747 | accuracy: 0.7500\n",
      "Batch: 380/475 | Test loss: 0.5372 | accuracy: 0.9375\n",
      "Batch: 475/475 | Test loss: 0.5607 | accuracy: 0.9375\n",
      "Test loss: 0.5362 acc: 0.8804\n",
      "\n",
      "\n",
      "Final Evaluation on Test Set:\n",
      "Test Loss: 0.5362\n",
      "Test Accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./results/r4_alpha12/checkpoints/model_step_500_final.pt\"\n",
    "model = setup_lora_model(base_model, r=4, alpha=12)\n",
    "model = load_checkpoint(model, None, checkpoint_path, resume_training=False)\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, return_values=True)\n",
    "print(f\"\\nFinal Evaluation on Test Set:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772410a8-ba54-47da-85bc-6efc6a68431e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

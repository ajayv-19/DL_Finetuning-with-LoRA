{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f9c226-dc47-4bc5-a373-3ccd003fbaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruchitjathania/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification, RobertaConfig\n",
    "# from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4466e73-1c2e-4070-8155-aec756c8ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "4 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "batch_size_val = 4 if device == \"cpu\" else 16 if device == \"cuda\" else 4\n",
    "num_workers_val = 4 if device == \"cuda\" else 0\n",
    "print(batch_size_val, num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d7e46f-1855-4f91-a7d4-5205f371e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "base_model = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset('ag_news', split='train', cache_dir='./data/')\n",
    "test_dataset = load_dataset('ag_news', split='test', cache_dir='./data/')\n",
    "\n",
    "# Extract the number of classess and their names\n",
    "num_labels = test_dataset.features['label'].num_classes\n",
    "class_names = test_dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Rename label column\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(tokenized_train, batch_size=batch_size_val, shuffle=True, collate_fn=data_collator, num_workers=num_workers_val)\n",
    "test_dataloader = DataLoader(tokenized_test, batch_size=batch_size_val, shuffle=False, collate_fn=data_collator, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d635f6a2-4c49-4428-a7c4-3fb5e5ffb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, targets):\n",
    "  predictions = torch.log_softmax(y_pred, dim=1).argmax(dim=1)\n",
    "  accuracy = (predictions == targets).sum() / len(targets)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae1ca18-a074-44ae-8f3e-8f260f33ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, checkpoint_path, resume_training=True):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if resume_training:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        step = checkpoint.get('step', 0)\n",
    "        epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"Resumed training from epoch {epoch}, step {step}\")\n",
    "        return model, optimizer, step, epoch\n",
    "    else:\n",
    "        print(\"Model loaded for inference\")\n",
    "        return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58edaeca-8398-483b-b10d-8952507a19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "\n",
    "def setup_lora_model(base_model, r, alpha):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        id2label=id2label,\n",
    "        cache_dir=\"./model_dir\"\n",
    "    )\n",
    "\n",
    "    # Freeze base model params\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define LoRA layers\n",
    "    class LoRALayer(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim, r, alpha):\n",
    "            super().__init__()\n",
    "            self.A = nn.Parameter(torch.empty(r, in_dim))\n",
    "            self.B = nn.Parameter(torch.empty(out_dim, r))\n",
    "            torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "            torch.nn.init.zeros_(self.B)\n",
    "            self.scaling = alpha / r\n",
    "\n",
    "        def forward(self, x):\n",
    "            A = self.A.to(x.device)\n",
    "            B = self.B.to(x.device)\n",
    "            return self.scaling * (x @ A.T @ B.T)\n",
    "\n",
    "    class LinearWithLoRA(nn.Module):\n",
    "        def __init__(self, linear, r, alpha):\n",
    "            super().__init__()\n",
    "            self.linear = linear\n",
    "            self.lora = LoRALayer(linear.in_features, linear.out_features, r, alpha)\n",
    "\n",
    "            # add bias terms:\n",
    "            if linear.bias is not None:\n",
    "                linear.bias.requires_grad = True\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear(x) + self.lora(x)\n",
    "\n",
    "    assign_lora = partial(LinearWithLoRA, r=r, alpha=alpha)\n",
    "\n",
    "    # Adding interjecting parameters to other layers:\n",
    "    # for layer in model.roberta.encoder.layer:\n",
    "    #     layer.attention.self.query = assign_lora(layer.attention.self.query)\n",
    "    #     layer.attention.self.value = assign_lora(layer.attention.self.value)\n",
    "    #     layer.output.dense = assign_lora(layer.output.dense)\n",
    "\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        if i >= 8:\n",
    "            layer.attention.self.query = assign_lora(layer.attention.self.query)\n",
    "            layer.attention.self.value = assign_lora(layer.attention.self.value)\n",
    "            # Optional: output dense layer\n",
    "            layer.output.dense = assign_lora(layer.output.dense)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d02f8f3-7363-4e85-b7f2-98fcbd91ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "# model = setup_lora_model(base_model, r=20, alpha=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f5f4c6-d753-4d59-8e07-048a26d18581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, return_values=False):\n",
    "    interval = len(test_loader) // 5 if len(test_loader) >= 5 else 1\n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            acc = get_accuracy(logits, labels)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_acc += acc.item()\n",
    "\n",
    "            if (batch_idx + 1) % interval == 0:\n",
    "                print(f\"Batch: {batch_idx+1}/{len(test_loader)} | Test loss: {loss:.4f} | accuracy: {acc:.4f}\")\n",
    "\n",
    "    test_loss = total_test_loss / len(test_loader)\n",
    "    test_acc = total_test_acc / len(test_loader)\n",
    "\n",
    "    print(f\"Test loss: {test_loss:.4f} acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "    if return_values:\n",
    "        return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6ec7b52-66b1-4327-a426-74ae9295a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(train_data, test_data, title, ylabel, save_path_base):\n",
    "    steps, train_vals = zip(*train_data)\n",
    "    _, test_vals = zip(*test_data)\n",
    "\n",
    "    # Plot with interactivity\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(steps, train_vals, label='Train')\n",
    "    ax.plot(steps, test_vals, label='Test')\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Save interactive plot as HTML\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        import plotly.io as pio\n",
    "\n",
    "        fig_plotly = go.Figure()\n",
    "        fig_plotly.add_trace(go.Scatter(x=steps, y=train_vals, mode='lines+markers', name='Train'))\n",
    "        fig_plotly.add_trace(go.Scatter(x=steps, y=test_vals, mode='lines+markers', name='Test'))\n",
    "        fig_plotly.update_layout(title=title, xaxis_title='Steps', yaxis_title=ylabel)\n",
    "        interactive_path = save_path_base.replace(\".png\", \".html\")\n",
    "        pio.write_html(fig_plotly, file=interactive_path, auto_open=False)\n",
    "    except ImportError:\n",
    "        print(\"Plotly not installed — skipping interactive plot save.\")\n",
    "\n",
    "    # Save static plot\n",
    "    static_path = save_path_base.replace(\".html\", \".png\")\n",
    "    fig.savefig(static_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save raw data\n",
    "    df = pd.DataFrame({\n",
    "        \"Step\": steps,\n",
    "        \"Train\": train_vals,\n",
    "        \"Test\": test_vals\n",
    "    })\n",
    "    csv_path = save_path_base.replace(\".png\", \"_data.csv\").replace(\".html\", \"_data.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved plot to {static_path} and interactive/data files to {interactive_path if 'interactive_path' in locals() else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3e35ed-945a-4688-b4e2-a4734394ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unlabelled(model, data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits  # [B, num_classes]\n",
    "            predictions = torch.argmax(logits, dim=1)  # get predicted class indices\n",
    "            preds.append(predictions.cpu())\n",
    "\n",
    "    return torch.cat(preds, dim=0)  # combine into a single tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274857f1-a8e9-4738-8774-7cb4a874db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, lr_scheduler, save_every_steps=200, output_dir=\"./checkpoints\", epochs=None, max_steps=None):\n",
    "    import time, os, torch\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    recent_checkpoints = []\n",
    "\n",
    "    total_training_time = 0\n",
    "    total_steps = 0\n",
    "    epoch = 0\n",
    "    train_losses, train_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    log_every_steps = 500  # how often to log + evaluate\n",
    "\n",
    "    print(\"Starting training...\\n\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    while True:\n",
    "        if epochs is not None and epoch >= epochs:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        epoch_steps = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            if max_steps is not None and total_steps >= max_steps:\n",
    "                break\n",
    "\n",
    "            step_start = time.time()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            acc = get_accuracy(logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_steps += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            if total_steps % log_every_steps == 0 or total_steps == max_steps:\n",
    "                avg_train_loss = epoch_loss / epoch_steps\n",
    "                avg_train_acc = epoch_acc / epoch_steps\n",
    "                train_losses.append((total_steps, avg_train_loss))\n",
    "                train_accuracies.append((total_steps, avg_train_acc))\n",
    "            \n",
    "                # Evaluate on test set\n",
    "                test_loss, test_acc = evaluate(model, test_dataloader, return_values=True)\n",
    "                test_losses.append((total_steps, test_loss))\n",
    "                test_accuracies.append((total_steps, test_acc))\n",
    "            \n",
    "            loss.backward()\n",
    "            # Grad Norm Clipping: \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "            \n",
    "            step_end = time.time()\n",
    "            step_time = step_end - step_start\n",
    "\n",
    "            if total_steps % 250 == 0 or total_steps == 1:\n",
    "                avg_loss = epoch_loss / epoch_steps\n",
    "                avg_acc = epoch_acc / epoch_steps\n",
    "                print(f\"[Step {total_steps}] Avg Loss (epoch): {avg_loss:.4f} | Avg Acc (epoch): {avg_acc:.4f} | Step Time: {step_time:.2f}s\")\n",
    "\n",
    "            if total_steps % save_every_steps == 0 or total_steps == max_steps:\n",
    "                ckpt_path = os.path.join(output_dir, f\"model_step_{total_steps}.pt\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'step': total_steps,\n",
    "                    'epoch': epoch,\n",
    "                }, ckpt_path)\n",
    "\n",
    "                print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "                recent_checkpoints.append(ckpt_path)\n",
    "                if len(recent_checkpoints) > 1:\n",
    "                    old_ckpt = recent_checkpoints.pop(0)\n",
    "                    if os.path.exists(old_ckpt):\n",
    "                        os.remove(old_ckpt)\n",
    "                        print(f\"Old checkpoint removed: {old_ckpt}\")\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        # Per-epoch average loss & accuracy\n",
    "        avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0\n",
    "        avg_epoch_acc = epoch_acc / epoch_steps if epoch_steps > 0 else 0\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  Avg Accuracy: {avg_epoch_acc:.4f}\")\n",
    "        print(f\"  Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        if max_steps is not None and total_steps >= max_steps:\n",
    "            print(f\"Reached max_steps={max_steps}, stopping training.\")\n",
    "            break\n",
    "    ckpt_path = os.path.join(output_dir, f\"model_step_{total_steps}_final.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step': total_steps,\n",
    "        'epoch': epoch,\n",
    "    }, ckpt_path)\n",
    "    print(f\"Final checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "    overall_time = time.time() - training_start\n",
    "    print(f\"Training completed in {epoch} epoch(s)\")\n",
    "    print(f\"Total training time: {overall_time:.2f}s\")\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6b863d-f215-4648-8ed9-880571ccbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60d563b-fdd7-4554-a152-d6d70550405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 8000/8000 [00:01<00:00, 4661.94 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LoRA r14_alpha28 ===\n",
      "Trainable Parameter Count: 989956\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k_/5_g7n9956sx0v0nprxy_dd3r0000gn/T/ipykernel_83417/4227988651.py:41: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Scalar.cpp:23.)\n",
      "  epoch_loss += loss.item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Avg Loss (epoch): 1.3106 | Avg Acc (epoch): 0.5000 | Step Time: 5.01s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m get_scheduler(\n\u001b[1;32m     30\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     32\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     33\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39mnum_training_steps\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m ckpt_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoints/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m train_losses, train_accuracies, test_losses, test_accuracies \u001b[38;5;241m=\u001b[39m train(model, train_dataloader, optimizer, save_every_steps\u001b[38;5;241m=\u001b[39msave_every_steps, output_dir\u001b[38;5;241m=\u001b[39mckpt_dir, epochs\u001b[38;5;241m=\u001b[39mmax_epochs, max_steps\u001b[38;5;241m=\u001b[39mmax_steps, lr_scheduler\u001b[38;5;241m=\u001b[39mlr_scheduler)\n\u001b[1;32m     38\u001b[0m evaluate(model, test_dataloader, return_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Inference for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, save_every_steps, output_dir, epochs, max_steps)\u001b[0m\n\u001b[1;32m     34\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     38\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss, outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     40\u001b[0m acc \u001b[38;5;241m=\u001b[39m get_accuracy(logits, labels)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1322\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[1;32m   1323\u001b[0m     input_ids,\n\u001b[1;32m   1324\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1325\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1326\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1327\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1328\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1329\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1330\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1331\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1332\u001b[0m )\n\u001b[1;32m   1333\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1334\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:978\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    976\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 978\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    979\u001b[0m     embedding_output,\n\u001b[1;32m    980\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    981\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    982\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    983\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    984\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    985\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    986\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    987\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    988\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    989\u001b[0m )\n\u001b[1;32m    990\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    991\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         output_attentions,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    632\u001b[0m         hidden_states,\n\u001b[1;32m    633\u001b[0m         attention_mask,\n\u001b[1;32m    634\u001b[0m         layer_head_mask,\n\u001b[1;32m    635\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    636\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    637\u001b[0m         past_key_value,\n\u001b[1;32m    638\u001b[0m         output_attentions,\n\u001b[1;32m    639\u001b[0m     )\n\u001b[1;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    510\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    521\u001b[0m         hidden_states,\n\u001b[1;32m    522\u001b[0m         attention_mask,\n\u001b[1;32m    523\u001b[0m         head_mask,\n\u001b[1;32m    524\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    525\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    439\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    446\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 447\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    448\u001b[0m         hidden_states,\n\u001b[1;32m    449\u001b[0m         attention_mask,\n\u001b[1;32m    450\u001b[0m         head_mask,\n\u001b[1;32m    451\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    452\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    453\u001b[0m         past_key_value,\n\u001b[1;32m    454\u001b[0m         output_attentions,\n\u001b[1;32m    455\u001b[0m     )\n\u001b[1;32m    456\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    457\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/DL_proj2/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:370\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    368\u001b[0m )\n\u001b[0;32m--> 370\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    371\u001b[0m     query_layer,\n\u001b[1;32m    372\u001b[0m     key_layer,\n\u001b[1;32m    373\u001b[0m     value_layer,\n\u001b[1;32m    374\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    375\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    376\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    377\u001b[0m )\n\u001b[1;32m    379\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    380\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "lora_r_values = [14, 14, 14]\n",
    "lora_alpha_values = [28, 36, 48]\n",
    "max_steps = None\n",
    "save_every_steps = 1500\n",
    "max_epochs = 4\n",
    "if max_epochs is not None:\n",
    "    num_training_steps = len(train_dataloader) * max_epochs\n",
    "else:\n",
    "    num_training_steps = max_steps\n",
    "\n",
    "\n",
    "# Load unlabelled test set\n",
    "unlabelled_df = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "tokenized_unlabelled = unlabelled_df.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_unlabelled.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "unlabelled_loader = DataLoader(tokenized_unlabelled, batch_size=batch_size_val, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "for r, alpha in zip(lora_r_values, lora_alpha_values):\n",
    "    tag = f\"r{r}_alpha{alpha}\"\n",
    "    print(f\"\\n=== Training LoRA {tag} ===\")\n",
    "    model = setup_lora_model(base_model, r, alpha)\n",
    "    print(\"Trainable Parameter Count: {}\".format(count_parameters(model)))\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    ckpt_dir = f\"./results/{tag}/checkpoints/\"\n",
    "    train_losses, train_accuracies, test_losses, test_accuracies = train(model, train_dataloader, optimizer, save_every_steps=save_every_steps, output_dir=ckpt_dir, epochs=max_epochs, max_steps=max_steps, lr_scheduler=lr_scheduler)\n",
    "    evaluate(model, test_dataloader, return_values=False)\n",
    "    print(f\"=== Inference for {tag} ===\")\n",
    "    inference_start = time.time()\n",
    "    preds = evaluate_unlabelled(model, unlabelled_loader)\n",
    "\n",
    "    output_dir = \"./results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"inference_output_{tag}.csv\")\n",
    "\n",
    "    df_output = pd.DataFrame({\n",
    "      'ID': range(len(preds)),\n",
    "      'Label': preds.numpy()\n",
    "    })\n",
    "    df_output.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "    inference_end = time.time()\n",
    "    inference_time = inference_end - inference_start\n",
    "    print(f\"Inference time: {inference_time:.2f}s\")\n",
    "    \n",
    "    plot_dir = f\"./results/{tag}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    plot_metrics(train_losses, test_losses, f\"{tag} - Loss\", \"Loss\", f\"./results/{tag}/loss_plot_{tag}.png\")\n",
    "    plot_metrics(train_accuracies, test_accuracies, f\"{tag} - Accuracy\", \"Accuracy\", f\"./results/{tag}/accuracy_plot_{tag}.png\")\n",
    "\n",
    "    # Cleanup after inference\n",
    "    del train_losses, train_accuracies, test_losses, test_accuracies\n",
    "    del model\n",
    "    del optimizer\n",
    "    del preds\n",
    "    del df_output\n",
    "    torch.cuda.empty_cache()  # safe to call on CPU too\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e9126-11ae-47c9-9310-b1a2e44d4c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
